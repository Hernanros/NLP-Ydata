{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "NLP_HW2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hernanros/NLP-Ydata/blob/master/NLP_HW2.ipynb/HW2\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb1yXLWqNO3s",
        "colab_type": "text"
      },
      "source": [
        "##  Text generation by Markov chain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf77BtrtNO3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIXpb5jBNO3w",
        "colab_type": "text"
      },
      "source": [
        "Markov chain is a probabalistic model in which the probability of each event depends only on the state attained in the previous event. [markovify](https://github.com/jsvine/markovify) is a library for text generation by Markov chain.\n",
        "\n",
        "Use \"pip install markovify\" to install markovify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d89JFOANO3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import markovify"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xclcdcfIO9t0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "11524786-5371-4c14-b413-382ae5372be4"
      },
      "source": [
        "!wget https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection/download"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-21 07:57:36--  https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection/download\n",
            "Resolving www.kaggle.com (www.kaggle.com)... 35.244.233.98\n",
            "Connecting to www.kaggle.com (www.kaggle.com)|35.244.233.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /account/login?returnUrl=%2Frmisra%2Fnews-headlines-dataset-for-sarcasm-detection%2Fdata [following]\n",
            "--2020-05-21 07:57:36--  https://www.kaggle.com/account/login?returnUrl=%2Frmisra%2Fnews-headlines-dataset-for-sarcasm-detection%2Fdata\n",
            "Reusing existing connection to www.kaggle.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘download’\n",
            "\n",
            "\rdownload                [<=>                 ]       0  --.-KB/s               \rdownload                [ <=>                ]   9.49K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-05-21 07:57:37 (604 KB/s) - ‘download’ saved [9718]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3dtkH9VNgf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parseJson(fname):\n",
        "    for line in open(fname, 'r+'):\n",
        "        yield eval(line)\n",
        "\n",
        "data = pd.DataFrame(list(parseJson('Sarcasm_Headlines_Dataset_v2.json')))\n",
        "\n",
        "\n",
        "\n",
        "data.is_sarcastic = data.is_sarcastic.astype('bool')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9yfEfD-QNSX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "baee0966-1ff8-4be5-8ec2-0e096b4189de"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "      <th>article_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>dem rep. totally nails why congress is falling...</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   is_sarcastic  ...                                       article_link\n",
              "0             1  ...  https://www.theonion.com/thirtysomething-scien...\n",
              "1             0  ...  https://www.huffingtonpost.com/entry/donna-edw...\n",
              "2             0  ...  https://www.huffingtonpost.com/entry/eat-your-...\n",
              "3             1  ...  https://local.theonion.com/inclement-weather-p...\n",
              "4             1  ...  https://www.theonion.com/mother-comes-pretty-c...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UYA9GlERvkc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "4ec91566-eca5-4e8f-d751-3bea2260283b"
      },
      "source": [
        "len([headline for i,headline in enumerate(data.headline) if data.is_sarcastic[i]==True])"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13634"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yolq4AkYNO30",
        "colab_type": "text"
      },
      "source": [
        "Download [Dataset.csv](https://drive.google.com/file/d/1raxIkJZ4lMTvgTB8eYjxu4Ux8aNL-x0s/view?usp=sharing) composed of sarcastic and serious headlines for the news. The csv-file consists of two columns. \"headline\" column contains texts of headlines. \"is_sarcastic\" column contain 0 if the hiadline is serious and 1 otherwise.\n",
        "\n",
        "Read dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_vDl5b8NO31",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "75b14128-407b-45d1-98ab-1fe179e452a1"
      },
      "source": [
        "texts_serious = [headline for i,headline in enumerate(data.headline) if data.is_sarcastic[i]==False] # list of serious hiadline texts\n",
        "texts_sarcastic = [headline for i,headline in enumerate(data.headline) if data.is_sarcastic[i]==True] # list of sarcastic hiadline texts\n",
        "# with open('Dataset.csv', encoding='utf-8') as f:\n",
        "#     # Rread csv-file by DictReader from csv library\n",
        "#     reader = csv.DictReader(f) \n",
        "#     for line in reader:\n",
        "#         # read texts of headline\n",
        "#         headline = line['headline'].strip()\n",
        "#         # read sarcasticity of headline\n",
        "#         is_sarcastic = int(line['is_sarcastic'].strip())\n",
        "#         if is_sarcastic:\n",
        "#             texts_sarcastic.append(headline)\n",
        "#         else:\n",
        "#             texts_serious.append(headline)\n",
        "print('Found {} sarcastic texts in Dataset'.format(data.groupby('is_sarcastic').size()[1]))\n",
        "print('Found {} serious texts in Dataset'.format(data.groupby('is_sarcastic').size()[0]))\n"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 13634 sarcastic texts in Dataset\n",
            "Found 14985 serious texts in Dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhtJNgwuNO35",
        "colab_type": "text"
      },
      "source": [
        "Merge headlines into one text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI9k6oOaNO35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_0 = '\\n '.join(texts_serious)\n",
        "text_1 = '\\n '.join(texts_sarcastic)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XextATaPUinJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e97e62ac-395d-4974-aba9-e0af78e496c6"
      },
      "source": [
        "markovify.Text(texts_sarcastic)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<markovify.text.Text at 0x7fd5b23312e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgh0nuOHNO38",
        "colab_type": "text"
      },
      "source": [
        "Create Markov chain model for serious headlines generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFez6CN-NO38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "serious_model = markovify.Text(texts_serious, state_size=2)\n",
        "sarcastic_model = markovify.Text(texts_sarcastic)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zZMwvZFNO3_",
        "colab_type": "text"
      },
      "source": [
        "Generate 20 serious headlines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDKt79ZcNO3_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "2e941942-83f5-4587-f1bb-8fc9405f547a"
      },
      "source": [
        "for i in range(20):\n",
        "    print(serious_model.make_sentence())"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "watch: how belgium became a surprise concert in nyc\n",
            "an open letter on behalf of all time\n",
            "7 clever ways to jump start your day\n",
            "5 easy breakfast bowls that are worthy of mocking\n",
            "big coal funded this prominent climate change threatens the newest prescription for children: time outdoors\n",
            "will content marketing strategies to pull in more lone children\n",
            "27 delicious ways to breakup like a penis\n",
            "second-guessing obama's foreign policy approval drops in new york and ended up on these 9 useful prime day for kids of color would be so much better to buy gold\n",
            "nana finally makes it too easy to point out the cutest award of oscar night\n",
            "mike trout really is the youngest business owner faces death threats after cosby routine\n",
            "why runners can't stop saying these ridiculous phrases at work\n",
            "colleges suspend students for sexual violence crisis\n",
            "4 simple ways to make japanese milk bread at home\n",
            "trump's whole approach to health care and immigration order overhauls\n",
            "friday's morning email: stormy daniels\n",
            "late night hosts gave donald trump rally after holding up pocket constitutions\n",
            "donald trump casts doubt on russian election interference ahead of u.s. government over dismal education\n",
            "anna faris was dropping hints about trouble with chris pratt so much, she totes around a cutout of him laughing hours before his death from cliff trying to tell the world we would before obamacare\n",
            "the secret key to setting achievable goals\n",
            "debbie wasserman schultz is taking over instagram\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDQHlJiFNO4B",
        "colab_type": "text"
      },
      "source": [
        "Create model and generate 20 sarcastic headlines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQOc0cWHNO4C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "4ea8b212-4b40-4761-922d-a1f9d5116c1b"
      },
      "source": [
        "for i in range(20):\n",
        "    print(sarcastic_model.make_sentence())"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dying baboon pretty low on civilians to oppress\n",
            "network executive cancels show after ruining it in new essay portion of every single personal problem reduces anxiety\n",
            "scientists finally figure out exactly who they really want to direct\n",
            "70 percent of americans would like to love a child\n",
            "couple dressed as wizard\n",
            "report: students who fail out of card table, sheet\n",
            "report: takeout place put burrito in completely different type of person no one at ad agency remembers hiring carrot top for man to say what's on his case about practicing his buckets\n",
            "7 total randos found dead by now\n",
            "jewelry company jumps gun with night vision laser scope if he weren't rich\n",
            "humane society volunteer spends whole day dreading fun activity he signed up based on mutual love of homeboys in outer space episode\n",
            "new historical evidence suggests president george washington sent woodcut of penis to prove presidential member completely normal\n",
            "boise homemaker bows toward mecca just to keep cat company\n",
            "market rallies after fed chief shows off huge wad of cash at trump rally after supporters clash with protesting gop leaders move goalposts on opposing trump to vatican city\n",
            "son needs so much about weasels\n",
            "trump denies use of organic peanut butter adds two minutes to vague up poem\n",
            "all of man's life\n",
            "mother surprised son needs so much more they getting out of way\n",
            "knocked-out secret service agent to stop dumping tap water down drain\n",
            "john kelly hoses layer of plywood to shuttered small businesses\n",
            "regular on sandy hook truth forum complaining about recent decline in bruce springsteen on fence about playing assad's birthday gig\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ5Jwl8TNO4E",
        "colab_type": "text"
      },
      "source": [
        "## Text generation by Variation Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K63RFOc9NO4E",
        "colab_type": "text"
      },
      "source": [
        "This part of tutorial based on [Text generation with a Variational Autoencoder](https://nicgian.github.io/text-generation-vae/) article. For sentence generation we use Variational Autoencoder (VAE) neural network model that is an extension seq2seq model. Originally VAE was described in [Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf) paper. The idea behind Variational Autoencoder is that we impose predefined disribution (e.g., normal distribution) on the latent state formed by encoder. On the one hand this restriction alow us to sample random vectors from normal distribution and generate arbitrary sentences. On the othe hand this restriction form very dense well differentiated space without holes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6gGt8yRNO4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "\n",
        "from keras.layers import Bidirectional, Dense, Embedding, \\\n",
        "Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers.advanced_activations import ELU\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "\n",
        "from scipy import spatial\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import codecs\n",
        "import random\n",
        "import csv\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG9qeYThNO4H",
        "colab_type": "text"
      },
      "source": [
        "Dowload [GloVe](http://nlp.stanford.edu/data/glove.6B.zip) pretrained word embedding vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wliKwdPug6qN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "3405a9aa-7905-409f-ee1a-969449401ce7"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-21 09:15:07--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-05-21 09:15:07--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-05-21 09:15:08--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.12MB/s    in 6m 30s  \n",
            "\n",
            "2020-05-21 09:21:38 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bkWe_Rnl-Gv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "f6a4202f-56af-4915-8c11-6a5cb75509a7"
      },
      "source": [
        "! unzip glove.6B.zip"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRO5eyfjNO4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 15 # Max text length in tokens\n",
        "MAX_NB_WORDS = 20000 # Max words in dictionary\n",
        "EMBEDDING_DIM = 50 # Dimensionality of GloVe vectors \n",
        "\n",
        "GLOVE_EMBEDDING = 'glove.6B.50d.txt'\n",
        "\n",
        "train_data, val_data = train_test_split(data, test_size = .25, random_state= 42)\n",
        "train_data.to_csv('train_data.csv')\n",
        "val_data.to_csv('val_data.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZIhktGyNO4J",
        "colab_type": "text"
      },
      "source": [
        "Create sentence tokenizer and two dictionaries: word_to_id and id_to_word\n",
        "\n",
        "For tokenisation we use [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) from keras library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUan2IgCNO4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#your code here\n",
        "\n",
        "tokenizer = Tokenizer(MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts_sarcastic)\n",
        "word_to_id = tokenizer.word_index \n",
        "word_to_id = {k:v-1 for k,v in  word_to_id.items()}\n",
        "id_to_word = {v:k for k,v in word_to_id.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGO1n0wyNO4L",
        "colab_type": "text"
      },
      "source": [
        "Tokenize sarcastic texts and create tensor composed of tokens indexes. If a sentence shorter than MAX_SEQUENCE_LENGTH we pad it. If a sentence longer than MAX_SEQUENCE_LENGTH we cut it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jsUsnDoNO4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#your code here\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts_sarcastic)\n",
        "sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "NB_WORDS = (min(tokenizer.num_words, len(word_to_id)) + 1 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhUP3Vz5jGGE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "9582c68e-fdad-4c50-def1-b12351f39476"
      },
      "source": [
        "sequences"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    0,     0,     0, ...,     2,   502,  1960],\n",
              "       [    0,     0,     0, ...,   100,     1,   101],\n",
              "       [    0,     0,     0, ...,   376, 10622,  4557],\n",
              "       ...,\n",
              "       [    0,     0,  4133, ...,     1,    10,   438],\n",
              "       [    0,     0,     0, ...,  1656,   603,  4404],\n",
              "       [    0,     0,     0, ...,    12,   230,   247]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZbZ5hKllVpH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "9bc9f6d6-f8e0-4e05-e9ad-160fe822e641"
      },
      "source": [
        "train_data"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "      <th>article_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>28580</th>\n",
              "      <td>False</td>\n",
              "      <td>pit bull lovers gather in washington</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/pit-bull-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14354</th>\n",
              "      <td>True</td>\n",
              "      <td>new tech-support caste arises in india</td>\n",
              "      <td>https://www.theonion.com/new-tech-support-cast...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19070</th>\n",
              "      <td>True</td>\n",
              "      <td>amazing original thing to become hated cliché ...</td>\n",
              "      <td>https://www.theonion.com/amazing-original-thin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5045</th>\n",
              "      <td>False</td>\n",
              "      <td>united airlines temporarily suspends cargo tra...</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/united-su...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24640</th>\n",
              "      <td>False</td>\n",
              "      <td>cuba and the united states: the long view</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/cuba-and-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21575</th>\n",
              "      <td>True</td>\n",
              "      <td>turnout lower than expected for gala central a...</td>\n",
              "      <td>https://www.theonion.com/turnout-lower-than-ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5390</th>\n",
              "      <td>True</td>\n",
              "      <td>retreating clinton campaign torches iowa town ...</td>\n",
              "      <td>https://politics.theonion.com/retreating-clint...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>True</td>\n",
              "      <td>national weather service to give hurricanes fu...</td>\n",
              "      <td>https://www.theonion.com/national-weather-serv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15795</th>\n",
              "      <td>True</td>\n",
              "      <td>christ returns for some of his old things</td>\n",
              "      <td>https://www.theonion.com/christ-returns-for-so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23654</th>\n",
              "      <td>True</td>\n",
              "      <td>loophole in curse lets archaeologist off the hook</td>\n",
              "      <td>https://www.theonion.com/loophole-in-curse-let...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>21464 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       is_sarcastic  ...                                       article_link\n",
              "28580         False  ...  https://www.huffingtonpost.com/entry/pit-bull-...\n",
              "14354          True  ...  https://www.theonion.com/new-tech-support-cast...\n",
              "19070          True  ...  https://www.theonion.com/amazing-original-thin...\n",
              "5045          False  ...  https://www.huffingtonpost.com/entry/united-su...\n",
              "24640         False  ...  https://www.huffingtonpost.com/entry/cuba-and-...\n",
              "...             ...  ...                                                ...\n",
              "21575          True  ...  https://www.theonion.com/turnout-lower-than-ex...\n",
              "5390           True  ...  https://politics.theonion.com/retreating-clint...\n",
              "860            True  ...  https://www.theonion.com/national-weather-serv...\n",
              "15795          True  ...  https://www.theonion.com/christ-returns-for-so...\n",
              "23654          True  ...  https://www.theonion.com/loophole-in-curse-let...\n",
              "\n",
              "[21464 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehtDdjiDNO4N",
        "colab_type": "text"
      },
      "source": [
        "Define batch generator to train a neural network\n",
        "\n",
        "For padding sentences to max length we use [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTj-u-QvNO4O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "345bf217-6ed8-42a9-9504-564ca3986039"
      },
      "source": [
        "def sent_generator(TRAIN_DATA_FILE, batchsize):\n",
        "    # Create iterator that reads dataset file batch by batch \n",
        "    #your code here\n",
        "    reader = pd.read_csv(TRAIN_DATA_FILE, chunksize= batchsize, iterator= True)\n",
        "    \n",
        "    for df in reader:\n",
        "\n",
        "        # Read a column that contains headlines\n",
        "        #your code here\n",
        "        texts = list(df['headline'])\n",
        "\n",
        "        # Tokenize texts and create padded tensor composed of tokens indexes\n",
        "        sequences = tokenizer.texts_to_sequences(texts)\n",
        "        sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "        \n",
        "        # Return input-target pairs\n",
        "        #your code here\n",
        "\n",
        "        yield (texts, sequences) \n",
        "\n",
        "print([a for a in sent_generator('train_data.csv', 4)][:3])"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(['pit bull lovers gather in washington', 'new tech-support caste arises in india', 'amazing original thing to become hated cliché in 6 months', 'united airlines temporarily suspends cargo travel for pets'], array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "         2956,  3911,  5541,  2010,     3,   910],\n",
            "       [    0,     0,     0,     0,     0,     0,     0,     0,     8,\n",
            "         3855,   567,  5712, 15779,     3,  2562],\n",
            "       [    0,     0,     0,     0,     0,  1965,  1498,   197,     1,\n",
            "         1042,  8249, 10241,     3,   335,   433],\n",
            "       [    0,     0,     0,     0,     0,     0,     0,  1116,  1906,\n",
            "         5460,  8562, 11333,  2108,     4,  5961]], dtype=int32)), (['cuba and the united states: the long view', 'chris stapleton had no idea who adele was when she covered his song', \"winter storm threatens homeless man's plans to survive over thanksgiving\", 'defiant milosevic eats big, sloppy sandwich during trial'], array([[   0,    0,    0,    0,    0,    0,    0, 8149,   33,   10, 1116,\n",
            "        1069,   10,  173, 2092],\n",
            "       [   0,    0,    0,    0, 1230,  229,   54,  279,   30,  103,   98,\n",
            "          87,  798,   40,  736],\n",
            "       [   0,    0,    0,    0,    0, 2270, 3832, 1567, 1465,  121,  283,\n",
            "           1, 4445,   55, 1346],\n",
            "       [   0,    0,    0,    0,    0,    0,    0, 2384, 4996, 1914,  151,\n",
            "        6945,  981,   89, 1271]], dtype=int32)), (['study: 58 percent of u.s. exercise televised', \"michael jordan displeased with this week's burnt offerings\", 'allowance to teach child importance of parental dependence', 'the one tip you need to achieve financial and physical health'], array([[    0,     0,     0,     0,     0,     0,     0,    49,  4381,\n",
            "          404,     2,    48,    57,  2750,  5001],\n",
            "       [    0,     0,     0,     0,     0,     0,     0,   469,  6201,\n",
            "         7554,     7,    43,  2529,  8069, 11922],\n",
            "       [    0,     0,     0,     0,     0,     0,     0,  9221,     1,\n",
            "         3706,   105,  9222,     2,  6149,  5431],\n",
            "       [    0,     0,     0,     0,    10,    35,  1481,    45,   887,\n",
            "            1,  3488,  1199,    33,  2645,   371]], dtype=int32))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ez-r3kaNO4Q",
        "colab_type": "text"
      },
      "source": [
        "Load pretrained GloVe vectors described in [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN0hxN0wNO4Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "9a67dedb-92f9-4b34-e0b0-8d034e3d129a"
      },
      "source": [
        "embeddings_index = {}\n",
        "with open('glove.6B.50d.txt', encoding='utf-8') as f:\n",
        "    # read rows from file line by line\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0] # Get word\n",
        "        coefs = np.asarray(values[1:], dtype='float32') # Get elements of word's vector\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUDA9KQzNO4T",
        "colab_type": "text"
      },
      "source": [
        "Create matrix from embedding vectors. Any row of the matrix is a word's vector. We get words from the dictionary word_to_id defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gkr2nZYUNO4U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "b6b5d1e5-ea17-4284-b49d-5d115613ee99"
      },
      "source": [
        "glove_embedding_matrix = np.zeros((NB_WORDS, EMBEDDING_DIM)) # Create empty matrix (max number of tokens, dimension of the embedding vectors)\n",
        "not_in_glove=0\n",
        "for word, i in word_to_id.items():\n",
        "    #your code here\n",
        "    if i<NB_WORDS:\n",
        "      try:\n",
        "        glove_embedding_matrix[i] = embeddings_index[word]\n",
        "      except:\n",
        "        glove_embedding_matrix[i] = embeddings_index['unk']\n",
        "        not_in_glove+=1\n",
        "        continue\n",
        "# compute number of words which there aren't in the GloVe vectors\n",
        "print(f'Null word embeddings: {not_in_glove}')"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Null word embeddings: 2389\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ1gaqKSNO4X",
        "colab_type": "text"
      },
      "source": [
        "Define parameters of the net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4rw7A3mNO4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "max_len = MAX_SEQUENCE_LENGTH\n",
        "emb_dim = EMBEDDING_DIM\n",
        "latent_dim = 32 # dimensionality of the hidden state in encoder and decoder RNN's\n",
        "intermediate_dim = 96 # dimensionality of variational space into which we map encoder's hidden state\n",
        "epsilon_std = 1.0 # standard deviation of gaussian noise\n",
        "act = ELU() # activation function of projection layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sarNC5v1NO4c",
        "colab_type": "text"
      },
      "source": [
        "Encoder of the variational autoencoder. It based on bidirectional LSTM\n",
        "\n",
        "We use following layers: [Input](https://www.tensorflow.org/api_docs/python/tf/keras/Input), [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding), [Bidirectional](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional), [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM), [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout), [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense), [ELU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ELU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgRQeNiGNO4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Input(batch_shape=(None, max_len)) # Input layer fo the net. \n",
        "# Write an embedding layer for the input sequences of indexes. \n",
        "# Use pretrained word embeddings as a embedding layer weights and don't update these weights\n",
        "\n",
        "#your code here\n",
        "x_embed = Embedding(NB_WORDS, emb_dim, weights=[glove_embedding_matrix],\n",
        "                            input_length=max_len, trainable=False)(x)\n",
        "\n",
        "\n",
        "# Bidirectional LSTM encoder\n",
        "\n",
        "#your code here\n",
        "h = Bidirectional(LSTM(intermediate_dim, return_sequences=False, recurrent_dropout=0.2), merge_mode='concat')(x_embed)\n",
        "\n",
        "\n",
        "\n",
        "h = Dropout(0.2)(h) # Dropout for the BiLSTM layer to avoid overfitting \n",
        "\n",
        "# Fully-connected layer to map encoder hidden state into variational space\n",
        "\n",
        "#your code here\n",
        "h = act(Dense(intermediate_dim, activation='linear')(h))\n",
        "\n",
        "h = Dropout(0.2)(h) # Dropout for the fully-connected layer to avoid overfitting \n",
        "z_mean = Dense(latent_dim)(h) # Fully-connected layer to map variational space into means space \n",
        "z_log_var = Dense(latent_dim)(h) # Fully-connected layer to map variational space into standard deviations space "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61_EJbujNO4f",
        "colab_type": "text"
      },
      "source": [
        "The mechanism for sampling hidden vectors from variational space\n",
        "\n",
        "To apply it to our model we use [Lambda](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda) layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5OGmTyuNO4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sampling(args):\n",
        "    # Vectors from means space and standard deviations space respectively\n",
        "    z_mean, z_log_var = args\n",
        "    # Sample random vectors from normal distribution with mean=0 and std=epsilon_std\n",
        "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
        "                              stddev=epsilon_std)\n",
        "    \n",
        "    #your code here\n",
        "    \n",
        "    # Get new hidden state for decoder using vectors from means, standard deviations and normal random spaces\n",
        "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "\n",
        "# Get hidden states for the decoder\n",
        "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U89BoV9NO4h",
        "colab_type": "text"
      },
      "source": [
        "Define decoder of the autoencoder\n",
        "\n",
        "For this we use following layers: [RepeatVector](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RepeatVector), [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM), [TimeDistributed](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed), [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kD2_yfJNO4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Repeat the hidden state vector to form input sequence for decoder\n",
        "repeated_context = RepeatVector(max_len)\n",
        "# Decoder LSTM\n",
        "decoder_h = LSTM(intermediate_dim, return_sequences=True, recurrent_dropout=0.2)\n",
        "# Layer for mapping from hidden satates space to the space of dimension equal to size of vocabulary\n",
        "decoder_mean = TimeDistributed(Dense(NB_WORDS, activation='linear'))\n",
        "# Generated sequence\n",
        "h_decoded = decoder_h(repeated_context(z))\n",
        "# Decode every time step vector of the decoded sequence into space of dimension equal to size of vocabulary\n",
        "x_decoded_mean = decoder_mean(h_decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f90V8-ENO4i",
        "colab_type": "text"
      },
      "source": [
        "Define layer for loss computing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjIXaSO8NO4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def zero_loss(y_true, y_pred):\n",
        "    # Return tensor filled with ones with shape equal generated sequence shape\n",
        "    return K.zeros_like(y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU1cXC0tNO4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomVariationalLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.is_placeholder = True\n",
        "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
        "        # Create tensor (batch_size, max_sequence_len) filled with ones to consider all elements of generated sequence \n",
        "        self.target_weights = tf.constant(np.ones((batch_size, max_len)), tf.float32)\n",
        "\n",
        "    def vae_loss(self, x, x_decoded_mean):\n",
        "        # Get tensor with similar shape as x\n",
        "        labels = tf.cast(x, tf.int32)\n",
        "        # Compute sequence reconstruction loss\n",
        "        xent_loss = K.sum(tfa.seq2seq.sequence_loss(x_decoded_mean, labels,\n",
        "                                                    weights=self.target_weights,\n",
        "                                                    average_across_timesteps=False,\n",
        "                                                    average_across_batch=False), axis=-1)\n",
        "        # Compute KL-divergence as Variational loss \n",
        "        kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "        # Composite loss (reconstruction loss + Variational loss)\n",
        "        return K.mean(xent_loss + kl_loss)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs[0] # input sequence\n",
        "        x_decoded_mean = inputs[1] # reconstructed sequence\n",
        "        print(x.shape, x_decoded_mean.shape)\n",
        "        loss = self.vae_loss(x, x_decoded_mean) # Compute loss of the model\n",
        "        self.add_loss(loss, inputs=inputs)\n",
        "        # we don't use this output, but it has to have the correct shape\n",
        "        return K.ones_like(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8GYYLM0NO4m",
        "colab_type": "text"
      },
      "source": [
        "Assemble the model\n",
        "\n",
        "To define model we use [Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) from tensorflow.\n",
        "\n",
        "To train model employ [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) optimization algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ9Zwt9DNO4n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "1bbcfe87-db38-4b61-a73f-74f024ccf901"
      },
      "source": [
        "# Create custom layer for loss computing\n",
        "loss_layer = CustomVariationalLayer()([x, x_decoded_mean])\n",
        "\n",
        "vae = Model(x, [loss_layer])\n",
        "# Use Adam optimizer with learning rate = 0.01\n",
        "opt = Adam(lr=0.01)\n",
        "vae.compile(optimizer='adam', loss=[zero_loss])\n",
        "# Show model structure\n",
        "vae.summary()"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 15) (64, 15, 20001)\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 15)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 15, 50)       1000050     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 192)          112896      embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 192)          0           bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 96)           18528       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "elu_2 (ELU)                     (None, 96)           0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 96)           0           elu_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 32)           3104        dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 32)           3104        dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 32)           0           dense_2[0][0]                    \n",
            "                                                                 dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_1 (RepeatVector)  (None, 15, 32)       0           lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 15, 96)       49536       repeat_vector_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 15, 20001)    1940097     lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "custom_variational_layer_1 (Cus [(None, 15), (None,  0           input_1[0][0]                    \n",
            "                                                                 time_distributed_1[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 3,127,315\n",
            "Trainable params: 2,127,265\n",
            "Non-trainable params: 1,000,050\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUq-wpywNO4r",
        "colab_type": "text"
      },
      "source": [
        "Checkpoint function to save states of our model during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N_2MDF_NO4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model_checkpoint(model_name):\n",
        "    filepath = f\"models/{model_name}.h5\"\n",
        "    directory = os.path.dirname(filepath)\n",
        "    try:\n",
        "        # Check if directory exists\n",
        "        os.stat(directory)\n",
        "    except:\n",
        "        # If directory doesn't exist, create the directory\n",
        "        os.mkdir(directory)\n",
        "    # Save model states\n",
        "    checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=False)\n",
        "    return checkpointer\n",
        "\n",
        "# Create model checkpointer\n",
        "checkpointer = create_model_checkpoint('vae_seq2seq')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Cw3RxnCNO4u",
        "colab_type": "text"
      },
      "source": [
        "Train model, test model after each apoch and save model's state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRTTpZOU5Zcf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "662b6c7f-e4fb-40b1-8396-9cd80df52f00"
      },
      "source": [
        "sents = val_data['headline']\n",
        "val_1_data = tokenizer.texts_to_sequences(sents)\n",
        "val_1_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "val_1_data\n",
        "pad_sequences([df for df in sent_generator('train_data.csv', batch_size)][0][0])"
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-276-dc6399e1f761>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mval_1_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_1_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'pit bull lovers gather in washington'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iscsBzrwNO4v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "7892fe5d-6024-42db-ebba-8d286a74df12"
      },
      "source": [
        "nb_epoch = 8 # number of epochs for model training\n",
        "n_steps = 28000 / batch_size # Number of steps per epoch\n",
        "for counter in range(nb_epoch):\n",
        "    print('-------epoch: ', counter, '-------')\n",
        "    # Train model. Test and save model after every epoch\n",
        "    vae.fit_generator(sent_generator('train_data.csv', batch_size),\n",
        "                      steps_per_epoch=n_steps, epochs=1, callbacks=[checkpointer],\n",
        "                      validation_data=(sents,val_1_data))\n",
        "vae.save(r'vae_lstmFull32dim96hid.h5')"
      ],
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------epoch:  0 -------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-273-022037953373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     vae.fit_generator(sent_generator('train_data.csv', batch_size),\n\u001b[1;32m      7\u001b[0m                       \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                       validation_data=(sents,val_1_data))\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'vae_lstmFull32dim96hid.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    148\u001b[0m                                      str(validation_data))\n\u001b[1;32m    149\u001b[0m                 val_x, val_y, val_sample_weights = model._standardize_user_data(\n\u001b[0;32m--> 150\u001b[0;31m                     val_x, val_y, val_sample_weight)\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 if model.uses_learning_phase and not isinstance(K.learning_phase(),\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (15,) but got array with shape (1,)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjXY9bJMNO4x",
        "colab_type": "text"
      },
      "source": [
        "Assemble encoder and decoder for sentence generation sampled from variational space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTJBfCHxNO4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make separate encoder to encode input sentence\n",
        "encoder = Model(x, z_mean)\n",
        "# Input layer for decoder to decode vectors sampled from variational space \n",
        "decoder_input = Input(shape=(latent_dim,))\n",
        "# Apply LSTM to decode hidden vector into sequence\n",
        "_h_decoded = decoder_h(repeated_context(decoder_input))\n",
        "# Decode every time step vector of the decoded sequence into space of dimension equal to size of vocabulary\n",
        "_x_decoded_mean = decoder_mean(_h_decoded)\n",
        "# Apply softmax to get most probable token\n",
        "_x_decoded_mean = Activation('softmax')(_x_decoded_mean)\n",
        "# Make decoderfor sempled sentences\n",
        "generator = Model(decoder_input, _x_decoded_mean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4a0y7FvNO4z",
        "colab_type": "text"
      },
      "source": [
        "Generate sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf7yrbD6NO4z",
        "colab_type": "code",
        "colab": {},
        "outputId": "11022952-843f-4ecd-fa0b-5ce9277a97bd"
      },
      "source": [
        "# Dictionary maps indexes to words\n",
        "index2word = {v: k for k, v in word_to_id.items()}\n",
        "# Fit sentences from validation set into encoder\n",
        "sent_encoded = encoder.predict(data_1_val, batch_size=16)\n",
        "# Decode encoded sentences\n",
        "x_test_reconstructed = generator.predict(sent_encoded)\n",
        "\n",
        "sent_idx = 400\n",
        "# Get words indexes with highest probability for the 500th sentence from validation set\n",
        "reconstructed_indexes = np.apply_along_axis(np.argmax, 1, x_test_reconstructed[sent_idx])\n",
        "# Map indexes of generated sentence to words\n",
        "word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
        "word_list = ' '.join([w for w in word_list if w])\n",
        "print(word_list)\n",
        "# Map indexes of input sentence to words\n",
        "original_sent = list(np.vectorize(index2word.get)(data_1_val[sent_idx]))\n",
        "original_sent = ' '.join([w for w in original_sent if w])\n",
        "print(original_sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'s to to to in\n",
            "lake bell welcomes baby girl\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}